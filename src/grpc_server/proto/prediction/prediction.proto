// Copyright 2020 kubeflow.org.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//    http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

syntax = "proto3";
package grpc_server;

service PredictionService
{
  rpc ServerLive(ServerLiveRequest) returns (ServerLiveResponse) {}
  rpc ServerReady(ServerReadyRequest) returns (ServerReadyResponse) {}
  rpc ModelReady(ModelReadyRequest) returns (ModelReadyResponse) {}
  rpc ServerMetadata(ServerMetadataRequest) returns (ServerMetadataResponse) {}
  rpc ModelMetadata(ModelMetadataRequest) returns (ModelMetadataResponse) {}
  rpc ModelInfer(ModelInferRequest) returns (ModelInferResponse) {}
  // galemind specific
  rpc ModelInferAsync(stream ModelInferRequest) returns (stream ModelInferResponse) {}

  // Enhanced unified inference with protocol support and streaming
  rpc UnifiedInfer(UnifiedInferRequest) returns (UnifiedInferResponse) {}
  rpc UnifiedInferStream(stream UnifiedInferRequest) returns (stream UnifiedInferResponse) {}
}

message ServerLiveRequest {}

message ServerLiveResponse
{
  bool live = 1;
}

message ServerReadyRequest {}

message ServerReadyResponse
{
  bool ready = 1;
}


message ModelReadyRequest
{
  // The name of the model to check for readiness.
  string name = 1;

  // The version of the model to check for readiness. If not given the
  // server will choose a version based on the model and internal policy.
  string version = 2;
}

message ModelReadyResponse
{
  bool ready = 1;
}

message ServerMetadataRequest {}

message ServerMetadataResponse
{
  string name = 1;

  string version = 2;

  repeated string extensions = 3;
}

message ModelMetadataRequest 
{
  string name = 1;

  string version = 2;
}

message ModelMetadataResponse
{
  message TensorMetadata
  {
    string name = 1;

    string datatype = 2;

    repeated int64 shape = 3;
  }

  string name = 1;

  repeated string versions = 2;

  string platform = 3;

  repeated TensorMetadata inputs = 4;

  repeated TensorMetadata outputs = 5;
}

message ModelInferRequest
{
  message InferInputTensor
  {
    string name = 1;

    string datatype = 2;

    repeated int64 shape = 3;

    map<string, InferParameter> parameters = 4;

    // The tensor contents using a data-type format. This field must
    // not be specified if "raw" tensor contents are being used for
    // the inference request.
    InferTensorContents contents = 5;
  }

  // An output tensor requested for an inference request.
  message InferRequestedOutputTensor
  {
    string name = 1;

    // Optional requested output tensor parameters.
    map<string, InferParameter> parameters = 2;
  }

  string model_name = 1;

  // The version of the model to use for inference. If not given the
  // server will choose a version based on the model and internal policy.
  string model_version = 2;

  // Optional identifier for the request. If specified will be
  // returned in the response.
  string id = 3;

  // Optional inference parameters.
  map<string, InferParameter> parameters = 4;

  // The input tensors for the inference.
  repeated InferInputTensor inputs = 5;

  // The requested output tensors for the inference. Optional, if not
  // specified all outputs produced by the model will be returned.
  repeated InferRequestedOutputTensor outputs = 6;

  // The data contained in an input tensor can be represented in "raw"
  // bytes form or in the repeated type that matches the tensor's data
  // type. To use the raw representation 'raw_input_contents' must be
  // initialized with data for each tensor in the same order as
  // 'inputs'. For each tensor, the size of this content must match
  // what is expected by the tensor's shape and data type. The raw
  // data must be the flattened, one-dimensional, row-major order of
  // the tensor elements without any stride or padding between the
  // elements. Note that the FP16 and BF16 data types must be represented as
  // raw content as there is no specific data type for a 16-bit float type.
  //
  // If this field is specified then InferInputTensor::contents must
  // not be specified for any input tensor.
  repeated bytes raw_input_contents = 7;
}

message ModelInferResponse
{
  message InferOutputTensor
  {
    string name = 1;

    string datatype = 2;

    repeated int64 shape = 3;

    // Optional output tensor parameters.
    map<string, InferParameter> parameters = 4;

    // The tensor contents using a data-type format. This field must
    // not be specified if "raw" tensor contents are being used for
    // the inference response.
    InferTensorContents contents = 5;
  }

  string model_name = 1;

  string model_version = 2;

  // The id of the inference request if one was specified.
  string id = 3;

  // Optional inference response parameters.
  map<string, InferParameter> parameters = 4;

  // The output tensors holding inference results.
  repeated InferOutputTensor outputs = 5;

  // The data contained in an output tensor can be represented in
  // "raw" bytes form or in the repeated type that matches the
  // tensor's data type. To use the raw representation 'raw_output_contents'
  // must be initialized with data for each tensor in the same order as
  // 'outputs'. For each tensor, the size of this content must match
  // what is expected by the tensor's shape and data type. The raw
  // data must be the flattened, one-dimensional, row-major order of
  // the tensor elements without any stride or padding between the
  // elements. Note that the FP16 and BF16 data types must be represented as
  // raw content as there is no specific data type for a 16-bit float type.
  //
  // If this field is specified then InferOutputTensor::contents must
  // not be specified for any output tensor.
  repeated bytes raw_output_contents = 6;
}

// An inference parameter value. The Parameters message describes a 
// “name”/”value” pair, where the “name” is the name of the parameter
// and the “value” is a boolean, integer, or string corresponding to 
// the parameter.
message InferParameter
{
  oneof parameter_choice
  {
    bool bool_param = 1;

    int64 int64_param = 2;

    double f64_param = 3;

    string string_param = 4;
  }
}

// The data contained in a tensor represented by the repeated type
// that matches the tensor's data type. Protobuf oneof is not used
// because oneofs cannot contain repeated fields.
message InferTensorContents
{
  // Representation for BOOL data type. The size must match what is
  // expected by the tensor's shape. The contents must be the flattened,
  // one-dimensional, row-major order of the tensor elements.
  repeated bool bool_contents = 1;

  // Representation for INT8, INT16, and INT32 data types. The size
  // must match what is expected by the tensor's shape. The contents
  // must be the flattened, one-dimensional, row-major order of the
  // tensor elements.
  repeated int32 int_contents = 2;

  // Representation for INT64 data types. The size must match what
  // is expected by the tensor's shape. The contents must be the
  // flattened, one-dimensional, row-major order of the tensor elements.
  repeated int64 int64_contents = 3;

  // Representation for UINT8, UINT16, and UINT32 data types. The size
  // must match what is expected by the tensor's shape. The contents
  // must be the flattened, one-dimensional, row-major order of the
  // tensor elements.
  repeated uint32 uint_contents = 4;

  // Representation for UINT64 data types. The size must match what
  // is expected by the tensor's shape. The contents must be the
  // flattened, one-dimensional, row-major order of the tensor elements.
  repeated uint64 uint64_contents = 5;

  // Representation for FP32 data type. The size must match what is
  // expected by the tensor's shape. The contents must be the flattened,
  // one-dimensional, row-major order of the tensor elements.
  repeated float fp32_contents = 6;

  // Representation for FP64 data type. The size must match what is
  // expected by the tensor's shape. The contents must be the flattened,
  // one-dimensional, row-major order of the tensor elements.
  repeated double fp64_contents = 7;

  // Representation for BYTES data type. The size must match what is
  // expected by the tensor's shape. The contents must be the flattened,
  // one-dimensional, row-major order of the tensor elements.
  repeated bytes bytes_contents = 8;
}

// Enhanced unified inference protocol support
enum InferenceProtocol {
  PROTOCOL_UNSPECIFIED = 0;
  PROTOCOL_GALEMIND = 1;
  PROTOCOL_OPENAI = 2;
}

// Content type for message payload
enum ContentType {
  CONTENT_TYPE_UNSPECIFIED = 0;
  CONTENT_TYPE_TEXT = 1;
  CONTENT_TYPE_BINARY = 2;
  CONTENT_TYPE_BASE64 = 3;
}

// Message content with different encoding options
message MessageContent {
  ContentType content_type = 1;

  oneof content {
    string text_content = 2;
    bytes binary_content = 3;
    string base64_content = 4;
  }
}

// Streaming metadata for chunk handling
message StreamMetadata {
  // Unique identifier for the stream session
  string stream_id = 1;

  // Chunk sequence number for ordering
  uint64 chunk_sequence = 2;

  // Flag indicating if this is a streaming message
  bool is_streaming = 3;

  // Flag indicating end of stream
  bool end_of_stream = 4;

  // Total number of expected chunks (optional, if known)
  optional uint64 total_chunks = 5;
}

// Enhanced unified inference request
message UnifiedInferRequest {
  // Protocol specification (defaults to Galemind for backward compatibility)
  InferenceProtocol protocol = 1;

  // Original ModelInferRequest for backward compatibility
  optional ModelInferRequest legacy_request = 2;

  // Enhanced message content
  MessageContent content = 3;

  // Streaming metadata
  optional StreamMetadata stream_metadata = 4;

  // Model identification
  string model_name = 5;
  string model_version = 6;

  // Request identification
  string request_id = 7;

  // Protocol-specific parameters
  map<string, InferParameter> parameters = 8;

  // Additional metadata for the request
  map<string, string> metadata = 9;
}

// Enhanced unified inference response
message UnifiedInferResponse {
  // Protocol used for this response
  InferenceProtocol protocol = 1;

  // Original ModelInferResponse for backward compatibility
  optional ModelInferResponse legacy_response = 2;

  // Enhanced response content
  MessageContent content = 3;

  // Streaming metadata
  optional StreamMetadata stream_metadata = 4;

  // Model identification
  string model_name = 5;
  string model_version = 6;

  // Request identification (echoed from request)
  string request_id = 7;

  // Response status and error information
  ResponseStatus status = 8;

  // Protocol-specific parameters
  map<string, InferParameter> parameters = 9;

  // Additional metadata for the response
  map<string, string> metadata = 10;

  // Performance metrics
  optional PerformanceMetrics metrics = 11;
}

// Response status information
message ResponseStatus {
  enum StatusCode {
    STATUS_UNKNOWN = 0;
    STATUS_SUCCESS = 1;
    STATUS_ERROR = 2;
    STATUS_TIMEOUT = 3;
    STATUS_RATE_LIMITED = 4;
    STATUS_INVALID_REQUEST = 5;
  }

  StatusCode code = 1;
  string message = 2;
  map<string, string> details = 3;
}

// Performance and usage metrics
message PerformanceMetrics {
  // Processing time in milliseconds
  uint64 processing_time_ms = 1;

  // Queue time in milliseconds
  uint64 queue_time_ms = 2;

  // Token usage (for LLM models)
  optional TokenUsage token_usage = 3;

  // Memory usage in bytes
  optional uint64 memory_usage_bytes = 4;

  // GPU utilization percentage (0-100)
  optional float gpu_utilization = 5;
}

// Token usage information for language models
message TokenUsage {
  uint32 prompt_tokens = 1;
  uint32 completion_tokens = 2;
  uint32 total_tokens = 3;
}
